[Node]
name = 
token = 

[Server]
url = http://51.178.142.183:5000

[LLM]
command = llama-server
gpu_layers = 99

[Models]
directory = C:/Users/pc00/AppData/Local/llama.cpp

