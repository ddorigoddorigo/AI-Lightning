[Node]
token = 
; Display name for the node (optional)
name = 

; === LIGHTNING WALLET (to receive direct payments) ===
; Configure your Lightning wallet to receive earnings directly
; Supported options:
; 1. Local LND - specify certificate and macaroon paths
; 2. Lightning Address (e.g.: yourname@getalby.com)
; If not configured, earnings go to the account balance on the server

[Lightning]
; Enable direct Lightning payments (true/false)
enabled = false
; LND REST URL (if using local LND)
lnd_rest_host = https://127.0.0.1:8080
; Path to TLS certificate
lnd_cert_path = ~/.lnd/tls.cert
; Path to macaroon (invoice to receive payments)
lnd_macaroon_path = ~/.lnd/data/chain/bitcoin/mainnet/invoice.macaroon

[Server]
URL = http://YOUR_SERVER_IP:5000

[LLM]
; Use 'llama-server' if installed in PATH (recommended)
; or specify the full path to the executable
command = llama-server
gpu_layers = 99
port_start = 11000
port_end = 12000

[Models]
; Directory for local GGUF models (optional)
directory = 

; === HUGGINGFACE MODELS (recommended) ===
; Format: hf_repo = owner/repo:quantization
; The model will be downloaded automatically from HuggingFace

[Model:llama3.2-1b]
hf_repo = bartowski/Llama-3.2-1B-Instruct-GGUF:Q4_K_M
context = 4096

[Model:qwen2.5-1.5b]
hf_repo = Qwen/Qwen2.5-1.5B-Instruct-GGUF:Q4_K_M
context = 4096

; === LOCAL MODELS (optional) ===
; Formato: path = percorso al file GGUF locale

; [Model:tinyllama-local]
; path = C:\llama.cpp\models\tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf
; context = 2048