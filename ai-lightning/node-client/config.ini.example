[Node]
token = 
; Nome visualizzato per il nodo (opzionale)
name = 

; === LIGHTNING WALLET (per ricevere pagamenti diretti) ===
; Configura il tuo wallet Lightning per ricevere i guadagni direttamente
; Opzioni supportate:
; 1. LND locale - specifica i percorsi di certificato e macaroon
; 2. Lightning Address (es: yourname@getalby.com)
; Se non configurato, i guadagni vanno nel balance dell'account sul server

[Lightning]
; Abilita pagamenti Lightning diretti (true/false)
enabled = false
; URL REST di LND (se usi LND locale)
lnd_rest_host = https://127.0.0.1:8080
; Percorso al certificato TLS
lnd_cert_path = ~/.lnd/tls.cert
; Percorso al macaroon (invoice per ricevere pagamenti)
lnd_macaroon_path = ~/.lnd/data/chain/bitcoin/mainnet/invoice.macaroon

[Server]
URL = http://YOUR_SERVER_IP:5000

[LLM]
; Usa 'llama-server' se è installato nel PATH (raccomandato)
; oppure specifica il path completo all'eseguibile
command = llama-server
gpu_layers = 99
port_start = 11000
port_end = 12000

[Models]
; Directory per modelli locali GGUF (opzionale)
directory = 

; === MODELLI HUGGINGFACE (raccomandato) ===
; Formato: hf_repo = owner/repo:quantizzazione
; Il modello sarà scaricato automaticamente da HuggingFace

[Model:llama3.2-1b]
hf_repo = bartowski/Llama-3.2-1B-Instruct-GGUF:Q4_K_M
context = 4096

[Model:qwen2.5-1.5b]
hf_repo = Qwen/Qwen2.5-1.5B-Instruct-GGUF:Q4_K_M
context = 4096

; === MODELLI LOCALI (opzionale) ===
; Formato: path = percorso al file GGUF locale

; [Model:tinyllama-local]
; path = C:\llama.cpp\models\tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf
; context = 2048